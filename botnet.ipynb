{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T14:16:23.715847700Z",
     "start_time": "2023-11-29T14:16:21.743335400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from halutmatmul.halutmatmul import HalutMatmul\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6d782978f29f273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T14:18:21.716453900Z",
     "start_time": "2023-11-29T14:18:21.673616600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV: 2175588it [00:14, 149919.60it/s]\n",
      "Reading CSV: 699094it [00:02, 260431.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_csv_data(file_path, num_lines=3000000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in tqdm(reader, desc=\"Reading CSV\"):\n",
    "            # print(type(row[0]))\n",
    "            if int(row[7]) < 8:\n",
    "                continue\n",
    "            labels.append(int(float(row[0])))  # 第一列是标签\n",
    "            features = [float(x) for x in row[1:]]  # 后面的列是特征\n",
    "            # indices = [1, 2] + list(range(5, 9)) + [] + list(range(17, 21)) + [23, 24, 27, 28, 31, 32, 35, 36, 39, 40] # 生成索引列表\n",
    "            # features = [float(row[i]) for i in indices]  # 选择的列是特征\n",
    "            data.append(features)\n",
    "            if len(data) >= num_lines:\n",
    "                break\n",
    "    return data, labels\n",
    "\n",
    "# 加载模型参数JSON文件\n",
    "# json_file = '../simulate/model_int.json'\n",
    "# json_file = '../simulate/model_oldz.json'\n",
    "\n",
    "\n",
    "# with open(json_file, 'r') as file:\n",
    "#     model_data = json.load(file)\n",
    "\n",
    "# 解析模型参数\n",
    "# layers_data = model_data['layers']\n",
    "\n",
    "# weights = []\n",
    "# biases = []\n",
    "\n",
    "# for layer_data in layers_data:\n",
    "#     weights.append(np.array(layer_data['weights']))\n",
    "#     biases.append(np.array(layer_data['biases']))\n",
    "\n",
    "# 创建模型实例\n",
    "# model = MultiLayerPerceptron(weights, biases)\n",
    "\n",
    "model_file = '../fingerprint_model.pth'\n",
    "device = torch.device('cpu')  # 将模型转移到CPU设备\n",
    "model_state_dict = torch.load(model_file, map_location=device)\n",
    "\n",
    "# 准备训练数据\n",
    "train_csv_file = '../dataset/fingerprint/unlimit/train_redeal.csv'  # 输入的 CSV 文件路径\n",
    "\n",
    "train_data, train_labels = read_csv_data(train_csv_file)\n",
    "\n",
    "# 准备测试数据\n",
    "test_csv_file = '../dataset/fingerprint/unlimit/test_redeal.csv'  # 输入的 CSV 文件路径\n",
    "test_data, test_labels = read_csv_data(test_csv_file)\n",
    "\n",
    "# train_csv_file = '../dataset/botnet/train_redeal.csv'  # 输入的 CSV 文件路径\n",
    "# train_data, train_labels = read_csv_data(train_csv_file,200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "633405ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_norm_layer(weight, bias, running_var, running_mean):\n",
    "    \"\"\"Adjusts the weight and bias of a normalization layer.\"\"\"\n",
    "    adjusted_weight = weight / np.sqrt(running_var + 1e-5)\n",
    "    adjusted_bias = bias - running_mean * adjusted_weight\n",
    "    return adjusted_weight, adjusted_bias\n",
    "\n",
    "# 索引列表\n",
    "layer_indexes = ['norm0', 'fc1', 'norm1', 'fc2', 'norm2', 'output']\n",
    "\n",
    "# 初始化权重和偏差列表\n",
    "weight = [model_state_dict[f'{index}.weight'].numpy().T for index in layer_indexes]\n",
    "bias = [model_state_dict[f'{index}.bias'].numpy() for index in layer_indexes]\n",
    "\n",
    "# 调整归一化层的权重和偏差\n",
    "for i, index in enumerate(layer_indexes):\n",
    "    if 'norm' in index:\n",
    "        weight[i], bias[i] = adjust_norm_layer(\n",
    "            weight[i], \n",
    "            bias[i], \n",
    "            model_state_dict[f'{index}.running_var'].numpy(), \n",
    "            model_state_dict[f'{index}.running_mean'].numpy()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_reset(x, num):\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            if num < 0:\n",
    "                x[i][j] = x[i][j] >> (-num) << (-num)\n",
    "\n",
    "\n",
    "def get_layers(data):\n",
    "    layers = []\n",
    "    fix_point = [-1, -2, -2, -1]\n",
    "\n",
    "    layers.append(np.array(data))\n",
    "\n",
    "    # layers.append((layers[-1] - model_state_dict['norm0.running_mean'].numpy()) / np.sqrt(model_state_dict['norm0.running_var'].numpy() + 1e-5) * weight[0] + bias[0])\n",
    "    layers.append(layers[-1] * weight[0] + bias[0])\n",
    "    layers.append(np.maximum(layers[-1], 0))\n",
    "\n",
    "    layers.append(np.dot(layers[-1], weight[1]) + bias[1])\n",
    "    # layers.append((layers[-1] - model_state_dict['norm1.running_mean'].numpy()) / np.sqrt(model_state_dict['norm1.running_var'].numpy() + 1e-5) * weight[2] + bias[2])\n",
    "    layers.append(layers[-1] * weight[2] + bias[2])\n",
    "    layers.append(np.maximum(layers[-1], 0))\n",
    "\n",
    "    layers.append(np.dot(layers[-1], weight[3]) + bias[3])\n",
    "    # layers.append((layers[-1] - model_state_dict['norm2.running_mean'].numpy()) / np.sqrt(model_state_dict['norm2.running_var'].numpy() + 1e-5) * weight[4] + bias[4])\n",
    "    layers.append(layers[-1] * weight[4] + bias[4])\n",
    "    layers.append(np.maximum(layers[-1], 0))\n",
    "\n",
    "    layers.append(np.dot(layers[-1], weight[5]) + bias[5])\n",
    "\n",
    "    return layers\n",
    "\n",
    "\n",
    "def reset_bias(hm, bias):\n",
    "    lut = hm.luts\n",
    "    for i in range(lut.shape[0]):\n",
    "        for j in range(lut.shape[2]):\n",
    "            lut[i][0][j] = lut[i][0][j] + bias[i]\n",
    "    return lut\n",
    "\n",
    "\n",
    "train_layers = get_layers(train_data)\n",
    "test_layers = get_layers(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41f0128b3816b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T14:21:11.798205400Z",
     "start_time": "2023-11-29T14:21:11.792209Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hm_1 = HalutMatmul(C=15, K=32)\n",
    "hm_1.learn_offline(train_layers[0], weight[0])\n",
    "lut1 = reset_bias(hm_1, model.biases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_2 = HalutMatmul(C=32, K=32)\n",
    "hm_2.learn_offline(train_layers[1], model.weights[1].T)\n",
    "lut2 = reset_bias(hm_2, model.biases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning simple k-means prototypes (2101216, 16)\n",
      "Initializing simple k-means prototypes with zero\n",
      "Training PQ slice 0/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.06 s\n",
      "  Iteration 24 (0.00 s, search 0.00 s): objective=18.0746 imbalance=8.891 nsplit=0       \n",
      "Training PQ slice 1/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.06 s\n",
      "  Iteration 24 (0.01 s, search 0.00 s): objective=45.8367 imbalance=1.168 nsplit=0       \n",
      "Training PQ slice 2/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.00 s, search 0.00 s): objective=30.3381 imbalance=1.257 nsplit=0       \n",
      "Training PQ slice 3/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.00 s, search 0.00 s): objective=41.3259 imbalance=1.424 nsplit=0       \n",
      "Training PQ slice 4/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.06 s\n",
      "  Iteration 24 (0.01 s, search 0.00 s): objective=28.2987 imbalance=1.359 nsplit=0       \n",
      "Training PQ slice 5/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.00 s, search 0.00 s): objective=66.4669 imbalance=3.330 nsplit=0       \n",
      "Training PQ slice 6/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.00 s, search 0.00 s): objective=37.3457 imbalance=3.935 nsplit=0       \n",
      "Training PQ slice 7/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.01 s, search 0.00 s): objective=46.7653 imbalance=1.213 nsplit=0       \n",
      "Training PQ slice 8/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.00 s, search 0.00 s): objective=4.65516 imbalance=12.548 nsplit=2       \n",
      "Training PQ slice 9/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.01 s, search 0.00 s): objective=11.6262 imbalance=1.801 nsplit=0       \n",
      "Training PQ slice 10/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.00 s, search 0.00 s): objective=1.04912 imbalance=15.604 nsplit=8        \n",
      "Training PQ slice 11/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.01 s, search 0.00 s): objective=1014.4 imbalance=1.652 nsplit=0        \n",
      "Training PQ slice 12/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.01 s, search 0.00 s): objective=20.1212 imbalance=1.467 nsplit=0       \n",
      "Training PQ slice 13/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.01 s, search 0.01 s): objective=890.104 imbalance=2.815 nsplit=0       \n",
      "Training PQ slice 14/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.01 s, search 0.00 s): objective=44.5444 imbalance=1.692 nsplit=0       \n",
      "Training PQ slice 15/16\n",
      "Sampling a subset of 4096 / 2101216 for training\n",
      "Clustering 4096 points in 1D to 16 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 24 (0.00 s, search 0.00 s): objective=4.90411 imbalance=7.069 nsplit=1       \n",
      "centroids (16, 16, 1) (16, 32, 1) [[4.47904611e-08]\n",
      " [1.95160538e-01]\n",
      " [1.67423952e+00]\n",
      " [3.66746187e+00]\n",
      " [5.25582619e-02]\n",
      " [9.93131548e-02]\n",
      " [2.13444710e+00]\n",
      " [1.06346365e-02]\n",
      " [7.01281667e-01]\n",
      " [4.46275651e-01]\n",
      " [3.30807781e-03]\n",
      " [6.05011272e+00]\n",
      " [3.08016837e-01]\n",
      " [2.64534689e-02]\n",
      " [1.35309219e+00]\n",
      " [1.01724708e+00]]\n",
      "Done learning simple k-means prototypes\n",
      "Learning progress (2101216, 16)-16-32: 1/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 2/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 3/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 4/16 (17.333 GB)\n",
      "created empty bucket:  13\n",
      "Learning progress (2101216, 16)-16-32: 5/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 6/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 7/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 8/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 9/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 10/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 11/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 12/16 (17.333 GB)\n",
      "created empty bucket:  21\n",
      "Learning progress (2101216, 16)-16-32: 13/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 14/16 (17.333 GB)\n",
      "created empty bucket:  21\n",
      "Learning progress (2101216, 16)-16-32: 15/16 (17.333 GB)\n",
      "Learning progress (2101216, 16)-16-32: 16/16 (17.333 GB)\n",
      "X_error mse / X mean squared value:  0.0010386202 0.006435346 6.1960526 1.4579021\n",
      "Error to Original squared diff 6.1896195\n",
      "(16, 5, 19) float32 (2101216, 16) float32\n",
      "(2101216, 512) (2101216, 16)\n",
      "{'alpha': 1, 'copy_X': False, 'fit_intercept': False, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}\n",
      "After Ridge regression (2101216, 16)-16-32(17.679 GB)\n"
     ]
    }
   ],
   "source": [
    "hm_3 = HalutMatmul(C=16, K=32)\n",
    "hm_3.learn_offline(train_layers[-2], weight[5])\n",
    "lut3 = reset_bias(hm_3, bias[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53bcddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = hm_3.splits_lists.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f9319a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_3.splits_lists = temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_3.splits_lists = (hm_3.splits_lists * (2 ** 2)).astype(np.int8).astype(np.float32) / (2 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "56a944f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hm_3.splits_lists)):\n",
    "    hm_3.splits_lists[i, :, :-3] = (hm_3.splits_lists[i, :, :-3] - bias[4][i]) / weight[4][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测准确率: 95.7728%\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(hm_3.splits_lists)):\n",
    "#     hm_3.splits_lists[i] = (hm_3.splits_lists[i] - bias[4][i]) / weight[4][i]\n",
    "# hm_3.splits_lists = (hm_3.splits_lists - bias[4]) / weight[4]\n",
    "test_halt_3 = hm_3.matmul_online(test_layers[-4])\n",
    "predicted_labels = np.argmax(test_halt_3, axis=1)\n",
    "accuracy = np.mean(predicted_labels == test_labels) * 100\n",
    "print(f\"预测准确率: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_halt_1 = hm_1.matmul_online(np.array(test_data))\n",
    "test_halt_2 = hm_2.matmul_online(test_halt_1)\n",
    "test_halt_3 = hm_3.matmul_online(test_halt_2)\n",
    "\n",
    "predicted_labels = np.argmax(test_halt_3, axis=1)\n",
    "accuracy = np.mean(predicted_labels == test_labels) * 100\n",
    "print(f\"预测准确率: {accuracy:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40: \n",
    "# 6: log 64/2 = 6 \n",
    "# 35: 32 + 3 后面有三个是dim scale offset，后面两个没啥用，dim是用来比较的维数\n",
    "# 左开右闭\n",
    "hm_2.splits_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luts: (n, C, K), C个codebook, 每个codebook有K个centroid，luts里存的这个是centroid和对应向量乘后的值，n是矩阵乘法最终结果的维度\n",
    "# 64: 矩阵大小40*64 的64\n",
    "# 40: C=40 的40\n",
    "# 64: K=64 的64\n",
    "hm_1.luts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(hm, lut, x, y):\n",
    "    t_last = (2 ** x)\n",
    "    t_now = (2 ** y)\n",
    "    # 量化hm_1.luts / 4为8位定点数\n",
    "    hm.splits_lists = (hm.splits_lists * t_last).astype(np.int16).astype(np.float32) / t_last\n",
    "    hm.luts = np.round(lut * t_now).astype(np.int16).astype(np.float32) / t_now\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_1 = reset(hm_1, lut1, 0, 1)\n",
    "hm_2 = reset(hm_2, lut2, 1, 2)\n",
    "hm_3 = reset(hm_3, lut3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_1.splits_lists[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336655eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "def get_range(list, index, num, min, max):\n",
    "    if num == 5:\n",
    "        if max > min:\n",
    "            ans.append([min, max, index])\n",
    "        return\n",
    "    # if list[num][index] > max:\n",
    "    #     max = list[num][index]\n",
    "    # else:\n",
    "    #     min = list[num][index]\n",
    "    get_range(list, index * 2, num + 1, min, list[num][index])\n",
    "    get_range(list, index * 2 + 1, num + 1, list[num][index] + 1, max)\n",
    "\n",
    "get_range(hm_3.splits_lists[0], 0, 0, -1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbb765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_16 = (np.array(ans) * 4).astype(np.int16)\n",
    "feat_dict = {\"f1\": ans_16[1:]}\n",
    "key_bits = {\"1\": 16}\n",
    "key_encode_bits = {\"f1\": 14}\n",
    "\n",
    "# 左闭右开\n",
    "result = get_feature_table_entries(feat_dict, key_bits, key_encode_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_16 = (np.array(ans) * 4).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a730e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
